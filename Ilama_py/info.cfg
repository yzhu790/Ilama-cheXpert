How to Use:

Save Files: Save the code blocks above into four separate files named constants.py, vlm_handler.py, metadata_loader.py, and main_pipeline.py in the same directory.

Install Dependencies: Make sure you have llama-cpp-python, pandas, Pillow, and tqdm installed (pip install llama-cpp-python pandas Pillow tqdm). Remember to install llama-cpp-python with GPU/Metal support if applicable.

Run the Main Script: Execute the pipeline using main_pipeline.py from your terminal, providing the necessary arguments just like before.

python main_pipeline.py \
    --image_dir /path/to/your/CheXpert-v1.0-small/train \
    --gguf_model_path /path/to/your/llama3-llava-next-8b-q6-k.gguf \
    --mmproj_path /path/to/your/corresponding-mmproj-f16.gguf \
    --output_csv chexpert_reports_refactored.csv \
    --metadata_csv /path/to/your/CheXpert-v1.0-small/train.csv \
    --prompt "Your desired prompt here..." \
    --n_gpu_layers -1 \
    --skip_processed